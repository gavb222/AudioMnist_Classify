# Audiomnist_classify
This project builds networks to classify the AudioMNIST dataset into different digits. In the original paper (https://arxiv.org/pdf/1807.03418.pdf) there are two architectures: AudioNet (time domain) and AlexNet (using magnitude spectrograms as input). Both AlexNet and AudioNet are implemented here at present, as well as a version of AlexNet that operates on MFCCs as input features. As this repository is updated, more architectures will be added.

The repository is structured such that networks.py contains the model architectures, train_audionet.py contains the training script for AudioNet, and mnist_port.py contains code for structuring the dataset appropriately for training. The data is loaded with a modified version of TorchVision's ImageFolder dataset and dataloader classes, functional on .wav and .mp3 files. This dataloader structure is very useful for audio classification tasks, as the ImageFolder library is for vision classification tasks.

So far, I have found that all models perform similarly well after equivalent training, suggesting either that the choice of input features is not particularly important for a problem as small as this one, or that the model architecture is powerful enough to transform any set of features into a learnable context in their limited space. Given the small size of the problem, I expect that the model is able to generate a representation of the input data powerful enough to classify with in very few layers, so it is likely that a reduction in model size would be possible for models with denser input representations (e.g. MFCCs, or even magnitude spectrograms). AudioNet, which functions directly on a waveform, likely needs more nonlinearity to create that representation.

An alternative to the strict feed-forward networks implemented here could be an encoder-masker-decoder architecture, such as conv-tasnet (https://arxiv.org/pdf/1809.07454.pdf), originally implemented for source separation. In order to adapt it for classification, the TCN-based masker network could be employed on its own, with a MLP as a final classifier.
